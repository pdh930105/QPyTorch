{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0047d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "from torch.utils.cpp_extension import load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c84cef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = \"./qtorch/quant/\"\n",
    "if torch.cuda.is_available():\n",
    "    quant_cuda = load(\n",
    "        name=\"quant_cuda\",\n",
    "        sources=[\n",
    "            os.path.join(current_path, \"quant_cuda/quant_cuda.cpp\"),\n",
    "            os.path.join(current_path, \"quant_cuda/bit_helper.cu\"),\n",
    "            os.path.join(current_path, \"quant_cuda/sim_helper.cu\"),\n",
    "            os.path.join(current_path, \"quant_cuda/block_kernel.cu\"),\n",
    "            os.path.join(current_path, \"quant_cuda/float_kernel.cu\"),\n",
    "            os.path.join(current_path, \"quant_cuda/fixed_point_kernel.cu\"),\n",
    "            os.path.join(current_path, \"quant_cuda/quant.cu\"),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5cc56122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'quant_cuda' from '/home/pdh/.cache/torch_extensions/py38_cu118/quant_cuda/quant_cuda.so'>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_cuda.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e7ca5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([int('0x3f111111', 0), int('0x3fc00000', 0), int('0x40800000', 0)]).type(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccee782",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c047f19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -9.4045,   9.0723,   6.3809,  -5.1546, -10.6995,  20.6390,   9.6401,\n",
       "         18.1697,  -5.8983,   5.4089])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean=torch.Tensor([0]*10), std=torch.Tensor([10]*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7caf5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1477, 0.9345, 0.7838, 0.2859, 0.5894, 0.2157, 0.6833, 1.4652, 0.0089,\n",
      "        2.2961])\n",
      "a's exponent:  tensor([-2., -0., -0., -1., -0., -2., -0.,  1., -6.,  2.])\n"
     ]
    }
   ],
   "source": [
    "#a_float = a.view(torch.float32)\n",
    "#print(a)\n",
    "a_float = torch.randn([10])\n",
    "print(a_float.abs())\n",
    "a_float = a_float.abs()+ 1e-8\n",
    "print(\"a's exponent: \", torch.log2(a_float).ceil())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cf2e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1562, 0.8750, 0.7500, 0.3125, 0.6250, 0.2188, 0.7500, 1.5000, 0.0098,\n",
      "        2.5000], device='cuda:0')\n",
      "tensor([0.1562, 0.8750, 0.7500, 0.3125, 0.5000, 0.2188, 0.7500, 1.5000, 0.0078,\n",
      "        2.0000], device='cuda:0')\n",
      "tensor([0.1562, 0.8750, 0.7500, 0.3125, 0.5000, 0.2188, 0.7500, 1.5000, 0.0098,\n",
      "        2.0000], device='cuda:0')\n",
      "tensor([0.1562, 0.8750, 0.7500, 0.3125, 0.6250, 0.1875, 0.7500, 1.5000, 0.0098,\n",
      "        2.5000], device='cuda:0')\n",
      "tensor([0.1562, 0.8750, 0.7500, 0.3125, 0.6250, 0.2188, 0.7500, 1.5000, 0.0098,\n",
      "        2.5000], device='cuda:0')\n",
      "tensor([0.1562, 0.8750, 0.8750, 0.3125, 0.6250, 0.2188, 0.6250, 1.5000, 0.0078,\n",
      "        2.5000], device='cuda:0')\n",
      "tensor([0.1562, 0.8750, 0.7500, 0.3125, 0.5000, 0.2188, 0.6250, 1.5000, 0.0098,\n",
      "        2.0000], device='cuda:0')\n",
      "tensor([0.1562, 0.8750, 0.7500, 0.2500, 0.6250, 0.2188, 0.7500, 1.5000, 0.0098,\n",
      "        2.5000], device='cuda:0')\n",
      "tensor([0.1562, 0.8750, 0.8750, 0.3125, 0.6250, 0.2188, 0.7500, 1.5000, 0.0098,\n",
      "        2.5000], device='cuda:0')\n",
      "tensor([0.1562, 0.8750, 0.7500, 0.2500, 0.5000, 0.2188, 0.6250, 1.5000, 0.0078,\n",
      "        2.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(quant_cuda.block_quantize_stochastic(a_float.cuda(), 4, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aef9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "478d40d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu : tensor(-3.3881e-09, device='cuda:0')\n",
      "cpu : tensor(0., device='cuda:0')\n",
      "tensor([0.0065, 0.0009, 0.0003, 0.0047, 0.0097, 0.0041, 0.0070, 0.0017, 0.0052,\n",
      "        0.0054], device='cuda:0')\n",
      "tensor([0.0008, 0.0046, 0.0031, 0.0018, 0.0046, 0.0003, 0.0039, 0.0072, 0.0068,\n",
      "        0.0039], device='cuda:0')\n",
      "compare 0:10 : tensor([ 1.3053e-08,  3.0544e-08, -4.0847e-08,  1.5745e-08, -4.0309e-09,\n",
      "        -2.4185e-08,  1.1642e-08, -3.3790e-08, -1.9994e-08, -1.7695e-08],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cutex\n",
    "\n",
    "M, N, K = 100, 100, 10\n",
    "a = torch.rand((M, K), dtype=torch.float32).cuda() * 1/100\n",
    "b = torch.rand((K, N), dtype=torch.float32).cuda() * 1/100\n",
    "c = torch.empty((M, N), dtype=torch.float32).cuda()\n",
    "\n",
    "#a = torch.randint(1, 100, (M, K), dtype=torch.float32).cuda()\n",
    "#b = torch.randint(1, 100, (K, N), dtype=torch.float32).cuda()\n",
    "#c = torch.randint(1, 100, (M, N), dtype=torch.float32).cuda()\n",
    "\n",
    "\n",
    "kernels = cutex.SourceModule(r\"\"\"\n",
    "__global__ void matmul(Tensor<float, 2> *a, Tensor<float, 2> *b, Tensor<float, 2> *c, int M, int N, int K) {\n",
    "    int m = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int n = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float v = 0.f;\n",
    "    if (m >= M || n >= N) return;\n",
    "    for (int k = 0; k < K; ++k) {\n",
    "        v += (*a)[m][k] * (*b)[k][n];\n",
    "    }\n",
    "    (*c)[m][n] = v;\n",
    "}\n",
    "\"\"\",float_bits=32, int_bits=32)\n",
    "\n",
    "kernels.matmul(a, b, c, M, N, K, grid=(N // 32 + 1, M // 32 + 1), block=(32, 32, 1))\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "if not torch.allclose(c, torch.mm(a, b)):\n",
    "    print(\"gpu :\", (c - torch.mm(a,b)).mean())\n",
    "    print(\"cpu :\", (c - torch.mm(a.cpu(), b.cpu()).cuda()).mean())\n",
    "    print(a[0, 0:10])\n",
    "    print(b[0:10, 0])\n",
    "    print(\"compare 0:10 :\", (c - torch.mm(a,b))[0,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8a927a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cutex\n",
    "\n",
    "\n",
    "\n",
    "bit_helper = cutex.SourceModule(r\"\"\"\n",
    "#define FLOAT_TO_BITS(x) (*reinterpret_cast<unsigned int*>(x))\n",
    "#define BITS_TO_FLOAT(x) (*reinterpret_cast<float*>(x))\n",
    "\n",
    "__device__ __forceinline__ unsigned int extract_exponent(float *a) {\n",
    "  unsigned int temp = *(reinterpret_cast<unsigned int*>(a));\n",
    "  temp = (temp << 1 >> 24); // single preciision, 1 sign bit, 23 mantissa bits\n",
    "  return temp-127+1; // exponent offset and virtual bit\n",
    "}\n",
    "\n",
    "__device__ __forceinline__ unsigned int round_bitwise_stochastic(unsigned int target,\n",
    "                                                                 unsigned int rand_prob,\n",
    "                                                                 int man_bits) {\n",
    "    unsigned int mask = (1 << (23-man_bits)) - 1;\n",
    "    unsigned int add_r = target+(rand_prob & mask);\n",
    "    unsigned int quantized = add_r & ~mask;\n",
    "    return quantized;\n",
    "}\n",
    "\n",
    "__device__ __forceinline__ unsigned int round_bitwise_nearest(unsigned int target,\n",
    "                                                              int man_bits) {\n",
    "    unsigned int mask = (1 << (23-man_bits)) - 1;\n",
    "    unsigned int rand_prob = 1 << (23-man_bits-1);\n",
    "    unsigned int add_r = target+rand_prob;\n",
    "    unsigned int quantized = add_r & ~mask;\n",
    "    return quantized;\n",
    "}\n",
    "\n",
    "__device__ __forceinline__ unsigned int clip_exponent(int exp_bits, int man_bits,\n",
    "                                                      unsigned int old_num,\n",
    "                                                      unsigned int quantized_num) {\n",
    "  if (quantized_num == 0)\n",
    "    return quantized_num;\n",
    "\n",
    "  int quantized_exponent_store = quantized_num << 1 >> 1 >> 23; // 1 sign bit, 23 mantissa bits\n",
    "  int max_exponent_store = (1 << (exp_bits - 1)) + 127; // we are not reserving an exponent bit for infinity, nan, etc\n",
    "  // Clippping Value Up\n",
    "  if (quantized_exponent_store > max_exponent_store)\n",
    "  {\n",
    "    unsigned int max_man = (unsigned int)-1 << 9 >> 9 >> (23 - man_bits) << (23 - man_bits); // 1 sign bit, 8 exponent bits, 1 virtual bit\n",
    "    unsigned int max_num = ((unsigned int)max_exponent_store << 23) | max_man;\n",
    "    unsigned int old_sign = old_num >> 31 << 31;\n",
    "    quantized_num = old_sign | max_num;\n",
    "  }\n",
    "  return quantized_num;\n",
    "}\n",
    "\n",
    "\n",
    "__device__ __forceinline__ unsigned int clip_max_exponent(int man_bits,\n",
    "                                                          unsigned int max_exponent,\n",
    "                                                          unsigned int quantized_num) {\n",
    "  unsigned int quantized_exponent = quantized_num << 1 >> 24 << 23; // 1 sign bit, 23 mantissa bits\n",
    "  if (quantized_exponent > max_exponent) {\n",
    "    unsigned int max_man = (unsigned int ) -1 << 9 >> 9 >> (23-man_bits) << (23-man_bits); // 1 sign bit, 8 exponent bits\n",
    "    unsigned int max_num = max_exponent | max_man;\n",
    "    unsigned int old_sign = quantized_num >> 31 << 31;\n",
    "    quantized_num = old_sign | max_num;\n",
    "  }\n",
    "  return quantized_num;\n",
    "}\n",
    "\"\"\",float_bits=32, int_bits=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66ee095e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.8954, 2.2833, 2.9487,  ..., 3.2966, 1.7785, 3.2356],\n",
      "        [1.6707, 1.8414, 2.6955,  ..., 2.4816, 1.2431, 2.4465],\n",
      "        [2.4314, 2.6764, 3.1013,  ..., 3.1104, 1.7638, 3.6102],\n",
      "        ...,\n",
      "        [2.5037, 1.5539, 1.9033,  ..., 2.5582, 1.5119, 2.4344],\n",
      "        [2.3465, 2.9766, 3.3992,  ..., 3.1431, 1.6484, 3.1594],\n",
      "        [3.1410, 2.5632, 1.7822,  ..., 2.8549, 2.5160, 2.6298]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "c-torch.mm(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9519d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
